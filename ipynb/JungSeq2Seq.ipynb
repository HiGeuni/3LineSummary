{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484906cb",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d2077869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import konlpy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537e08b",
   "metadata": {},
   "source": [
    "## preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "65e290a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def loadData(DATA_PATH):\n",
    "    with open(DATA_PATH, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    title = []\n",
    "    content = []\n",
    "    for i in data:\n",
    "        title.append(i[\"title\"])\n",
    "        content.append(i[\"content\"])\n",
    "    return title, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "647ca461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "    words = konlpy.tag.Okt().morphs(''.join(doc))\n",
    "    tokens = [word for word in words if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7e4bfe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(docs, min_len=1, stopwords=None, tokenizer=tokenizer_re):\n",
    "    words = set()\n",
    "    for doc in docs:\n",
    "        words |= {token for token in tokenizer(doc) if len(token)>min_len}\n",
    "    if stopwords is not None:\n",
    "        workds -= set(stopwords)\n",
    "    vocab = {word: idx for idx, word in enumerate(words)}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9225ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_word(vocab):\n",
    "    return {idx: word for (word, idx) in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0b7c31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장의 순서 뒤집어주는 함수\n",
    "def tokenizer_re(doc): \n",
    "    tokens = konlpy.tag.Okt().morphs(''.join(doc))[::-1]\n",
    "#     tokens.insert(0, \"<SOS>\") #start of sequence\n",
    "#     tokens.append(\"<EOS>\")\n",
    "    tokens = '<SOS>' + tokens +'<EOS>'\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f051928c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', '초콜릿', '가나', '아', '바사', '마', '가나다라', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "doc = \"가나다라 마바사아 가나 초콜릿\"\n",
    "re = tokenizer_re(doc)\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "bb1755a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a61f6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "target, docs = loadData(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "37b1d655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'개막': 0,\n",
       " '상당수': 1,\n",
       " '광역': 2,\n",
       " '제책': 3,\n",
       " '균등히': 4,\n",
       " '광주시': 5,\n",
       " '많아': 6,\n",
       " '겪게': 7,\n",
       " '인수': 8,\n",
       " '으로는': 9,\n",
       " '소년': 10,\n",
       " '모집': 11,\n",
       " '떨군': 12,\n",
       " '휩쓸며': 13,\n",
       " '성남혜': 14,\n",
       " '다루기': 15,\n",
       " '풍속': 16,\n",
       " '이뤄졌다': 17,\n",
       " '청운대': 18,\n",
       " 'Hall': 19,\n",
       " '0.1%': 20,\n",
       " '사석': 21,\n",
       " '지난주': 22,\n",
       " '잡겠다는': 23,\n",
       " '단어': 24,\n",
       " '끼리': 25,\n",
       " '필요성': 26,\n",
       " '맞선': 27,\n",
       " '추후': 28,\n",
       " '서귀포': 29,\n",
       " '중구': 30,\n",
       " '주신': 31,\n",
       " '대인': 32,\n",
       " '섬유': 33,\n",
       " '14,750원': 34,\n",
       " '평론가': 35,\n",
       " '신규': 36,\n",
       " '철쭉': 37,\n",
       " '남방': 38,\n",
       " '1.6': 39,\n",
       " '케일': 40,\n",
       " '기다려지는': 41,\n",
       " '터미널': 42,\n",
       " '2천': 43,\n",
       " '내비': 44,\n",
       " '7~8': 45,\n",
       " '시가': 46,\n",
       " '실정': 47,\n",
       " '한시': 48,\n",
       " '11억': 49,\n",
       " '판로': 50,\n",
       " '빠른': 51,\n",
       " '논란': 52,\n",
       " '진영': 53,\n",
       " '않았': 54,\n",
       " '엘사': 55,\n",
       " '부산시': 56,\n",
       " '계약해제': 57,\n",
       " '없는': 58,\n",
       " '우승할': 59,\n",
       " '함께': 60,\n",
       " '안전하며': 61,\n",
       " '28일': 62,\n",
       " '지를': 63,\n",
       " '네트워크': 64,\n",
       " '저어새': 65,\n",
       " '토사': 66,\n",
       " '49.0%': 67,\n",
       " '수업': 68,\n",
       " '제주도': 69,\n",
       " '알렸다': 70,\n",
       " '911': 71,\n",
       " '받은': 72,\n",
       " '츄럴': 73,\n",
       " '학술': 74,\n",
       " '최우수': 75,\n",
       " '사람과': 76,\n",
       " '인생': 77,\n",
       " '소유진': 78,\n",
       " '개혁': 79,\n",
       " '3천': 80,\n",
       " '아침': 81,\n",
       " '흘리': 82,\n",
       " '봉쇄': 83,\n",
       " '수상소감': 84,\n",
       " '지은': 85,\n",
       " '랭킹': 86,\n",
       " '예방': 87,\n",
       " '소임': 88,\n",
       " '팝스타': 89,\n",
       " 'KTB': 90,\n",
       " '방문객': 91,\n",
       " '화력발전': 92,\n",
       " '외치': 93,\n",
       " '감안': 94,\n",
       " '헤어': 95,\n",
       " '생활환경': 96,\n",
       " '과잉': 97,\n",
       " '장신구': 98,\n",
       " '643억': 99,\n",
       " '배정': 100,\n",
       " '여성인권': 101,\n",
       " '자세한': 102,\n",
       " '세월호': 103,\n",
       " '파악': 104,\n",
       " '사정없이': 105,\n",
       " '부르고': 106,\n",
       " '특별할': 107,\n",
       " '홍문기': 108,\n",
       " '시시하게': 109,\n",
       " '성적': 110,\n",
       " '참배': 111,\n",
       " '아동노동': 112,\n",
       " 'MRCC': 113,\n",
       " '말투': 114,\n",
       " '비만': 115,\n",
       " '고뇌': 116,\n",
       " '5%': 117,\n",
       " '포드': 118,\n",
       " '교통혼잡': 119,\n",
       " '표창장': 120,\n",
       " '타이': 121,\n",
       " '상설': 122,\n",
       " '일상': 123,\n",
       " '화한다는': 124,\n",
       " '여야': 125,\n",
       " '잔잔하게': 126,\n",
       " '10분': 127,\n",
       " '아카이브': 128,\n",
       " '1~3만원': 129,\n",
       " '했지만': 130,\n",
       " '최고': 131,\n",
       " '행복한': 132,\n",
       " '황홀한': 133,\n",
       " '종로': 134,\n",
       " '악보': 135,\n",
       " '조심스럽다': 136,\n",
       " '홈페이지': 137,\n",
       " '유학': 138,\n",
       " '푸른': 139,\n",
       " '시트': 140,\n",
       " '복귀': 141,\n",
       " '빠짐': 142,\n",
       " '쓰레기': 143,\n",
       " '날렸다': 144,\n",
       " '겨울왕국': 145,\n",
       " '레스터': 146,\n",
       " '연료': 147,\n",
       " '정주영': 148,\n",
       " '유진': 149,\n",
       " '건수': 150,\n",
       " '산화물': 151,\n",
       " '메모리즈': 152,\n",
       " '있다며': 153,\n",
       " '율곡': 154,\n",
       " '일본인': 155,\n",
       " '귀리': 156,\n",
       " '지방의회': 157,\n",
       " '하이패스': 158,\n",
       " '내게': 159,\n",
       " 'Aquaculture': 160,\n",
       " '찾았다': 161,\n",
       " '고민': 162,\n",
       " '경우': 163,\n",
       " '갭은': 164,\n",
       " '보장': 165,\n",
       " '먹거리': 166,\n",
       " '하겠다는': 167,\n",
       " '기준': 168,\n",
       " '파괴': 169,\n",
       " '판별': 170,\n",
       " '작품': 171,\n",
       " '관중': 172,\n",
       " '초를': 173,\n",
       " '지도': 174,\n",
       " '자산': 175,\n",
       " '영양소': 176,\n",
       " '풍요롭': 177,\n",
       " '불가피하게': 178,\n",
       " '뉴딜': 179,\n",
       " '시즌': 180,\n",
       " '이체': 181,\n",
       " '무엇': 182,\n",
       " '간절한': 183,\n",
       " '위주': 184,\n",
       " '2.5%': 185,\n",
       " '교육성': 186,\n",
       " '저축': 187,\n",
       " '동력': 188,\n",
       " '강동원': 189,\n",
       " '팜을': 190,\n",
       " '일구고': 191,\n",
       " '일해': 192,\n",
       " '20조': 193,\n",
       " '보정': 194,\n",
       " '억제': 195,\n",
       " '내놓': 196,\n",
       " '쉼표': 197,\n",
       " '900': 198,\n",
       " '대우': 199,\n",
       " '사용': 200,\n",
       " '작은': 201,\n",
       " '폐지': 202,\n",
       " '수상했다': 203,\n",
       " '파격': 204,\n",
       " '민관': 205,\n",
       " '감면': 206,\n",
       " '4.3%': 207,\n",
       " 'BMW': 208,\n",
       " '이나': 209,\n",
       " '7.6': 210,\n",
       " '연구실': 211,\n",
       " '예요': 212,\n",
       " '우천': 213,\n",
       " '세로': 214,\n",
       " '고아': 215,\n",
       " '영양실조': 216,\n",
       " '좌전': 217,\n",
       " '노동자': 218,\n",
       " '의약품': 219,\n",
       " '목적': 220,\n",
       " '국장': 221,\n",
       " '탄도': 222,\n",
       " '여유': 223,\n",
       " '여수시청': 224,\n",
       " '내기': 225,\n",
       " '쏠리고': 226,\n",
       " '응징': 227,\n",
       " '안되는': 228,\n",
       " '마지막': 229,\n",
       " '호의': 230,\n",
       " '전자기기': 231,\n",
       " 'David': 232,\n",
       " '한양대': 233,\n",
       " '도청': 234,\n",
       " '언어': 235,\n",
       " '누르고': 236,\n",
       " '시키려고': 237,\n",
       " '선량': 238,\n",
       " '불과한데도': 239,\n",
       " '1080': 240,\n",
       " '종로구': 241,\n",
       " '추적': 242,\n",
       " '간조': 243,\n",
       " '육거리': 244,\n",
       " '차재윤': 245,\n",
       " '성과': 246,\n",
       " '특화된': 247,\n",
       " '전기': 248,\n",
       " '초청': 249,\n",
       " '고전': 250,\n",
       " '입장권': 251,\n",
       " '물건': 252,\n",
       " '인필드플라이': 253,\n",
       " '과제': 254,\n",
       " '기와': 255,\n",
       " '33%': 256,\n",
       " '45분': 257,\n",
       " '파머스': 258,\n",
       " '시세': 259,\n",
       " '서울시': 260,\n",
       " '양형': 261,\n",
       " '3~2019': 262,\n",
       " '공도': 263,\n",
       " '노선': 264,\n",
       " '부족해': 265,\n",
       " '아무': 266,\n",
       " '지척': 267,\n",
       " '줄기세포': 268,\n",
       " '극장': 269,\n",
       " '내렸을': 270,\n",
       " '유출': 271,\n",
       " '7억': 272,\n",
       " '유적': 273,\n",
       " '박치': 274,\n",
       " '마타': 275,\n",
       " '부천시': 276,\n",
       " '설립': 277,\n",
       " '기관': 278,\n",
       " '만든': 279,\n",
       " '탈북': 280,\n",
       " '허겁지겁': 281,\n",
       " '매월': 282,\n",
       " '타운': 283,\n",
       " '거나': 284,\n",
       " '6.0%': 285,\n",
       " '알로하': 286,\n",
       " '딛고': 287,\n",
       " '철새': 288,\n",
       " '틸라피아': 289,\n",
       " '혁신': 290,\n",
       " '일본군': 291,\n",
       " '16시간': 292,\n",
       " '융합': 293,\n",
       " '스토리': 294,\n",
       " '수원': 295,\n",
       " '올린만큼': 296,\n",
       " '몰수': 297,\n",
       " '중소': 298,\n",
       " '국고': 299,\n",
       " '남게': 300,\n",
       " 'Boogie': 301,\n",
       " '이병': 302,\n",
       " '서해안': 303,\n",
       " '의정부시': 304,\n",
       " '조합원': 305,\n",
       " 'Lab': 306,\n",
       " '22': 307,\n",
       " '자랑': 308,\n",
       " '앞장서': 309,\n",
       " '피오': 310,\n",
       " '규정': 311,\n",
       " '가점': 312,\n",
       " '합동': 313,\n",
       " '발목': 314,\n",
       " '35': 315,\n",
       " '이뤄질': 316,\n",
       " '소울': 317,\n",
       " '늘어나고': 318,\n",
       " '금융투자': 319,\n",
       " '보급': 320,\n",
       " '국가주석': 321,\n",
       " '떨어지는': 322,\n",
       " '조사': 323,\n",
       " '생기': 324,\n",
       " '춘절': 325,\n",
       " '생긴': 326,\n",
       " '2.0%': 327,\n",
       " '속보': 328,\n",
       " '미묘한': 329,\n",
       " '사업자': 330,\n",
       " '도착': 331,\n",
       " '통역사': 332,\n",
       " '대숲': 333,\n",
       " '용도': 334,\n",
       " '안된다': 335,\n",
       " '장유정': 336,\n",
       " '놓으면': 337,\n",
       " '걸림': 338,\n",
       " '응대': 339,\n",
       " '나갔고': 340,\n",
       " '자진': 341,\n",
       " '형성': 342,\n",
       " '선정': 343,\n",
       " '정경': 344,\n",
       " '내다보며': 345,\n",
       " '도지사': 346,\n",
       " '최대한': 347,\n",
       " '만수르': 348,\n",
       " '간편식': 349,\n",
       " '출발점': 350,\n",
       " 'OCN': 351,\n",
       " '매일': 352,\n",
       " '대안': 353,\n",
       " '시티즌': 354,\n",
       " '7-16': 355,\n",
       " '동영상': 356,\n",
       " '피난': 357,\n",
       " '폭소': 358,\n",
       " '열었다고': 359,\n",
       " '않아': 360,\n",
       " '가능하도록': 361,\n",
       " '알리는': 362,\n",
       " '2회': 363,\n",
       " '특히': 364,\n",
       " '강직': 365,\n",
       " '빗대': 366,\n",
       " '키워내면서': 367,\n",
       " '않으며': 368,\n",
       " '줄었으며': 369,\n",
       " '고도': 370,\n",
       " '타종': 371,\n",
       " '임시': 372,\n",
       " '생활화': 373,\n",
       " '김종수': 374,\n",
       " '부족하다': 375,\n",
       " '밀어': 376,\n",
       " '안나': 377,\n",
       " '온주완': 378,\n",
       " '연상': 379,\n",
       " '영양학': 380,\n",
       " '대나무': 381,\n",
       " '폐업': 382,\n",
       " '서귀포시': 383,\n",
       " '활발하게': 384,\n",
       " '손자': 385,\n",
       " '깊은': 386,\n",
       " '학부모': 387,\n",
       " '임부': 388,\n",
       " '겨울': 389,\n",
       " '도무지': 390,\n",
       " '자생': 391,\n",
       " '예습': 392,\n",
       " '출마': 393,\n",
       " '레깅스': 394,\n",
       " '김관영': 395,\n",
       " '계량기': 396,\n",
       " '엄단': 397,\n",
       " '웃고': 398,\n",
       " '집결': 399,\n",
       " '열려': 400,\n",
       " '와의': 401,\n",
       " '포착': 402,\n",
       " '32.3%': 403,\n",
       " '서비스': 404,\n",
       " '급히': 405,\n",
       " '수묵': 406,\n",
       " '이나마': 407,\n",
       " '단체': 408,\n",
       " '1만원': 409,\n",
       " '자랑스러운': 410,\n",
       " '고문': 411,\n",
       " '잃고': 412,\n",
       " '줄어들었다': 413,\n",
       " '기계': 414,\n",
       " '강화한다고': 415,\n",
       " '부동': 416,\n",
       " '세액': 417,\n",
       " '구분': 418,\n",
       " '산란': 419,\n",
       " '완주군': 420,\n",
       " '별다른': 421,\n",
       " '우공이산': 422,\n",
       " '나가': 423,\n",
       " '안유진': 424,\n",
       " '바로': 425,\n",
       " '제휴': 426,\n",
       " '당당한': 427,\n",
       " '측면': 428,\n",
       " '112': 429,\n",
       " '입장': 430,\n",
       " '어울리는': 431,\n",
       " '열고': 432,\n",
       " '높게': 433,\n",
       " '루수': 434,\n",
       " '다저스': 435,\n",
       " '모습': 436,\n",
       " '절감': 437,\n",
       " '정경순': 438,\n",
       " '해석': 439,\n",
       " '국립공원': 440,\n",
       " '여름': 441,\n",
       " '즐겼다': 442,\n",
       " '편견': 443,\n",
       " '않는다는': 444,\n",
       " '특별한': 445,\n",
       " '화하도록': 446,\n",
       " '로써': 447,\n",
       " '활성화': 448,\n",
       " '종자': 449,\n",
       " '그런데도': 450,\n",
       " '외교': 451,\n",
       " '해리': 452,\n",
       " '상치': 453,\n",
       " '콘서트': 454,\n",
       " '화해': 455,\n",
       " '훈수': 456,\n",
       " '이해도': 457,\n",
       " '3%': 458,\n",
       " '결의': 459,\n",
       " '스쿼시': 460,\n",
       " '0.292': 461,\n",
       " '최근': 462,\n",
       " '1969년': 463,\n",
       " '관점': 464,\n",
       " '동반': 465,\n",
       " '압수수색': 466,\n",
       " '수준': 467,\n",
       " '정의': 468,\n",
       " '퀴즈': 469,\n",
       " '긴장': 470,\n",
       " '도심': 471,\n",
       " '전면': 472,\n",
       " '입자': 473,\n",
       " '진심': 474,\n",
       " '정치': 475,\n",
       " '갯벌': 476,\n",
       " '76억원': 477,\n",
       " '학습': 478,\n",
       " '윤후덕': 479,\n",
       " '무술': 480,\n",
       " '높이려고': 481,\n",
       " '스페셜': 482,\n",
       " '살아가는': 483,\n",
       " '모은': 484,\n",
       " '주유소': 485,\n",
       " '주워지면서': 486,\n",
       " '국민연금': 487,\n",
       " '생길': 488,\n",
       " '식민': 489,\n",
       " '휠소터란': 490,\n",
       " '부를': 491,\n",
       " '자금': 492,\n",
       " '해오다': 493,\n",
       " '친환경': 494,\n",
       " '부모님': 495,\n",
       " '있어': 496,\n",
       " '친할': 497,\n",
       " '조시': 498,\n",
       " ')·': 499,\n",
       " '2015년': 500,\n",
       " '조건': 501,\n",
       " '채운': 502,\n",
       " '브랜드': 503,\n",
       " '교착': 504,\n",
       " '이모': 505,\n",
       " '3.29': 506,\n",
       " '직결': 507,\n",
       " '루드': 508,\n",
       " '청장': 509,\n",
       " '언니': 510,\n",
       " '네이션': 511,\n",
       " '러닝': 512,\n",
       " '커다란': 513,\n",
       " '모듈': 514,\n",
       " '서명': 515,\n",
       " '계란': 516,\n",
       " '예쁘다': 517,\n",
       " '산림': 518,\n",
       " 'Place': 519,\n",
       " '밀집': 520,\n",
       " '교통': 521,\n",
       " '겪는': 522,\n",
       " '충만': 523,\n",
       " '드러냈다': 524,\n",
       " '프로': 525,\n",
       " '화가': 526,\n",
       " '간병인': 527,\n",
       " '매진': 528,\n",
       " '돼가고': 529,\n",
       " '대금': 530,\n",
       " '편리': 531,\n",
       " '겨루게': 532,\n",
       " '비디오테이프': 533,\n",
       " '갖춰': 534,\n",
       " '구호': 535,\n",
       " '용서': 536,\n",
       " '리더': 537,\n",
       " '해냈다': 538,\n",
       " '높이기': 539,\n",
       " '콤비': 540,\n",
       " 'DLS': 541,\n",
       " '면치': 542,\n",
       " '추진': 543,\n",
       " '교섭단체': 544,\n",
       " '선향': 545,\n",
       " '인위': 546,\n",
       " '시장': 547,\n",
       " '기억': 548,\n",
       " '급별': 549,\n",
       " '큰길': 550,\n",
       " '수목드라마': 551,\n",
       " '산청군': 552,\n",
       " '조만간': 553,\n",
       " '거쳐서': 554,\n",
       " '단절': 555,\n",
       " '조진태': 556,\n",
       " '화하겠다': 557,\n",
       " '생태': 558,\n",
       " '브로치': 559,\n",
       " '되기': 560,\n",
       " '내렸다': 561,\n",
       " '개별': 562,\n",
       " '조를': 563,\n",
       " '갖추기': 564,\n",
       " '전문가': 565,\n",
       " '24시간': 566,\n",
       " '해결': 567,\n",
       " '훌륭한': 568,\n",
       " '국토교통부': 569,\n",
       " '주차공간': 570,\n",
       " '노랑': 571,\n",
       " '2017-004616': 572,\n",
       " '장충': 573,\n",
       " '최고급': 574,\n",
       " '긴밀한': 575,\n",
       " '담배': 576,\n",
       " '건축': 577,\n",
       " '성찰': 578,\n",
       " '불법': 579,\n",
       " '청개구리': 580,\n",
       " '구민': 581,\n",
       " '노미': 582,\n",
       " '지하': 583,\n",
       " '문제점': 584,\n",
       " '도시화': 585,\n",
       " '강대': 586,\n",
       " '경옥': 587,\n",
       " '장미정원': 588,\n",
       " '나무숲': 589,\n",
       " 'MVP': 590,\n",
       " '영양분': 591,\n",
       " '덕진구': 592,\n",
       " '강민': 593,\n",
       " '평이': 594,\n",
       " '12-4': 595,\n",
       " '6천': 596,\n",
       " '들여다본': 597,\n",
       " '신제품': 598,\n",
       " '회복하고': 599,\n",
       " '붙읍시다': 600,\n",
       " '중시': 601,\n",
       " '첨예하게': 602,\n",
       " '108': 603,\n",
       " '변함이': 604,\n",
       " '살려야': 605,\n",
       " '충북': 606,\n",
       " '아픈': 607,\n",
       " '살아남은': 608,\n",
       " '중간지점': 609,\n",
       " '경전철': 610,\n",
       " '우지': 611,\n",
       " '응원': 612,\n",
       " '2012년': 613,\n",
       " '예비': 614,\n",
       " '얻지': 615,\n",
       " '부천': 616,\n",
       " '가고': 617,\n",
       " '격렬한': 618,\n",
       " '통행량': 619,\n",
       " '초선': 620,\n",
       " '전보': 621,\n",
       " '1980년': 622,\n",
       " '고품격': 623,\n",
       " '맞서는': 624,\n",
       " '폐쇄회': 625,\n",
       " '속옷': 626,\n",
       " '기재': 627,\n",
       " '높았다': 628,\n",
       " '달았다': 629,\n",
       " '폐기물': 630,\n",
       " '기록': 631,\n",
       " '어르신': 632,\n",
       " '93%': 633,\n",
       " '유니온': 634,\n",
       " '기다리고': 635,\n",
       " '사는': 636,\n",
       " '우울하거나': 637,\n",
       " '보조금': 638,\n",
       " '게스트': 639,\n",
       " 'Webster': 640,\n",
       " '있거나': 641,\n",
       " '9천만': 642,\n",
       " '박영선': 643,\n",
       " '칭하': 644,\n",
       " '복선': 645,\n",
       " '컴퍼니': 646,\n",
       " '갈등': 647,\n",
       " '갖춰져': 648,\n",
       " '가가': 649,\n",
       " '피노키오': 650,\n",
       " '담는': 651,\n",
       " '토론': 652,\n",
       " '오늘': 653,\n",
       " '뒤집기': 654,\n",
       " '토너먼트': 655,\n",
       " '걸음': 656,\n",
       " '주요': 657,\n",
       " '파도': 658,\n",
       " '전문': 659,\n",
       " '텔레파시': 660,\n",
       " '까지를': 661,\n",
       " '뷰티': 662,\n",
       " '근심': 663,\n",
       " '간편하게': 664,\n",
       " '안된다는': 665,\n",
       " '이희호': 666,\n",
       " '보여주고': 667,\n",
       " '현충일': 668,\n",
       " '포스코': 669,\n",
       " '508': 670,\n",
       " '이시종': 671,\n",
       " '미국': 672,\n",
       " '음료': 673,\n",
       " '동향': 674,\n",
       " 'Sridhar': 675,\n",
       " '노무현': 676,\n",
       " '인사': 677,\n",
       " '금요일': 678,\n",
       " '코오롱': 679,\n",
       " '플래시': 680,\n",
       " '보인다': 681,\n",
       " '원전': 682,\n",
       " '있었고': 683,\n",
       " '벗고': 684,\n",
       " '5분': 685,\n",
       " '과로': 686,\n",
       " '번영로': 687,\n",
       " '아울러': 688,\n",
       " '서구': 689,\n",
       " '명감': 690,\n",
       " '아니다': 691,\n",
       " '주변인': 692,\n",
       " '마쳤다': 693,\n",
       " '공문': 694,\n",
       " '사상': 695,\n",
       " '괴산': 696,\n",
       " '감사': 697,\n",
       " '아로마': 698,\n",
       " '새기는': 699,\n",
       " '로그': 700,\n",
       " '도로로': 701,\n",
       " '사태': 702,\n",
       " '인재': 703,\n",
       " '태세': 704,\n",
       " '소방': 705,\n",
       " '규범': 706,\n",
       " '습기': 707,\n",
       " '동남아시아': 708,\n",
       " '불매': 709,\n",
       " '인도인': 710,\n",
       " 'SKC': 711,\n",
       " '유용성': 712,\n",
       " '만나게': 713,\n",
       " '소비자원': 714,\n",
       " '망월동': 715,\n",
       " '4시': 716,\n",
       " '밀폐': 717,\n",
       " '78억달러': 718,\n",
       " '갖추고': 719,\n",
       " '가장이': 720,\n",
       " '특유': 721,\n",
       " '없었다고': 722,\n",
       " '경과': 723,\n",
       " '열매': 724,\n",
       " '회의장': 725,\n",
       " '김선': 726,\n",
       " '애플': 727,\n",
       " '선보이고': 728,\n",
       " '자원': 729,\n",
       " '장재': 730,\n",
       " '에서의': 731,\n",
       " '슬픔': 732,\n",
       " '현지': 733,\n",
       " '잔돈': 734,\n",
       " '일괄': 735,\n",
       " '어디': 736,\n",
       " '성신여대': 737,\n",
       " '5천만': 738,\n",
       " '상영': 739,\n",
       " '0.3': 740,\n",
       " '감액': 741,\n",
       " '새로워진': 742,\n",
       " '초콜릿': 743,\n",
       " '프로필': 744,\n",
       " '특징': 745,\n",
       " '이용호': 746,\n",
       " '암시': 747,\n",
       " '스랩': 748,\n",
       " '구좌읍': 749,\n",
       " '리뉴얼': 750,\n",
       " '옥천군': 751,\n",
       " '출산': 752,\n",
       " '불행한': 753,\n",
       " '경위': 754,\n",
       " '줬다': 755,\n",
       " '서는': 756,\n",
       " '83%': 757,\n",
       " '활발한': 758,\n",
       " '꺼내든': 759,\n",
       " '세부': 760,\n",
       " '연수': 761,\n",
       " '참작': 762,\n",
       " '출의': 763,\n",
       " '전자': 764,\n",
       " '15.2%': 765,\n",
       " '구를': 766,\n",
       " '이기택': 767,\n",
       " '무대': 768,\n",
       " '연령': 769,\n",
       " '그래도': 770,\n",
       " '고독': 771,\n",
       " '범행': 772,\n",
       " '감소': 773,\n",
       " '3300달러': 774,\n",
       " '합류': 775,\n",
       " '청마': 776,\n",
       " '장방': 777,\n",
       " '응급처치': 778,\n",
       " '그린': 779,\n",
       " '안전': 780,\n",
       " '외국': 781,\n",
       " '브루': 782,\n",
       " '변화': 783,\n",
       " '체험': 784,\n",
       " '농번기': 785,\n",
       " '업자': 786,\n",
       " '무단': 787,\n",
       " '승부': 788,\n",
       " '육성': 789,\n",
       " '15억원': 790,\n",
       " 'CBS': 791,\n",
       " '리얼미터': 792,\n",
       " '점도': 793,\n",
       " '제작': 794,\n",
       " '대통령령': 795,\n",
       " '표명': 796,\n",
       " '서약': 797,\n",
       " '추격': 798,\n",
       " 'WSJ': 799,\n",
       " '손안': 800,\n",
       " '잘사는': 801,\n",
       " '마치': 802,\n",
       " '간선': 803,\n",
       " '정당': 804,\n",
       " '식혀줄': 805,\n",
       " '터널': 806,\n",
       " '정보처리': 807,\n",
       " '세무': 808,\n",
       " '현안': 809,\n",
       " '스러': 810,\n",
       " '선제': 811,\n",
       " '진출': 812,\n",
       " '오히려': 813,\n",
       " '11월': 814,\n",
       " '3,115원': 815,\n",
       " '육류': 816,\n",
       " '잡았고': 817,\n",
       " '신경': 818,\n",
       " '했기': 819,\n",
       " '고려대': 820,\n",
       " '끌었다': 821,\n",
       " '있을': 822,\n",
       " '설전': 823,\n",
       " '10억': 824,\n",
       " '선두': 825,\n",
       " '어린이': 826,\n",
       " '산둥성': 827,\n",
       " '보살펴야': 828,\n",
       " '고구마': 829,\n",
       " '달여': 830,\n",
       " '석장리': 831,\n",
       " '시일': 832,\n",
       " '유치한': 833,\n",
       " '무시': 834,\n",
       " '내보내': 835,\n",
       " '영농': 836,\n",
       " '응급의료': 837,\n",
       " '위축': 838,\n",
       " '특화단': 839,\n",
       " '차원': 840,\n",
       " '밸리': 841,\n",
       " '연못': 842,\n",
       " '채무': 843,\n",
       " '썼으나': 844,\n",
       " '선택': 845,\n",
       " '제천시': 846,\n",
       " '안정': 847,\n",
       " '캐슬': 848,\n",
       " '26일': 849,\n",
       " '콘크리트': 850,\n",
       " '설득': 851,\n",
       " '나볼': 852,\n",
       " '휴대전화': 853,\n",
       " '712': 854,\n",
       " '내고': 855,\n",
       " '수면': 856,\n",
       " '치매': 857,\n",
       " '건너편': 858,\n",
       " '그간': 859,\n",
       " '51%': 860,\n",
       " '조짐': 861,\n",
       " '올여름': 862,\n",
       " '홍북읍': 863,\n",
       " '첫날': 864,\n",
       " '8년': 865,\n",
       " '사고': 866,\n",
       " '맞춰': 867,\n",
       " '확정': 868,\n",
       " '2014년': 869,\n",
       " '하더라': 870,\n",
       " '1,600': 871,\n",
       " '만들어지는': 872,\n",
       " '김재': 873,\n",
       " '비메': 874,\n",
       " '1천': 875,\n",
       " '되고': 876,\n",
       " '지나며서': 877,\n",
       " '의도': 878,\n",
       " '약품': 879,\n",
       " '골목': 880,\n",
       " '기획전': 881,\n",
       " '높지': 882,\n",
       " '본환': 883,\n",
       " '둘러보는': 884,\n",
       " '아팠는데': 885,\n",
       " '민심': 886,\n",
       " '단장': 887,\n",
       " '애민': 888,\n",
       " '터너': 889,\n",
       " '실생활': 890,\n",
       " 'Society': 891,\n",
       " '속임수': 892,\n",
       " '야생동물': 893,\n",
       " '밀레니엄': 894,\n",
       " '정취': 895,\n",
       " '태수': 896,\n",
       " '태양광': 897,\n",
       " '위생': 898,\n",
       " '바나나': 899,\n",
       " '14.4%': 900,\n",
       " '보면': 901,\n",
       " '입단': 902,\n",
       " '행실': 903,\n",
       " '불리는': 904,\n",
       " '서부': 905,\n",
       " '결과물': 906,\n",
       " '관리': 907,\n",
       " '행궁동': 908,\n",
       " '이상': 909,\n",
       " '레인': 910,\n",
       " '정원석': 911,\n",
       " '3100만원': 912,\n",
       " '게티': 913,\n",
       " '종이': 914,\n",
       " '47': 915,\n",
       " '대당': 916,\n",
       " '19일': 917,\n",
       " '프로그램': 918,\n",
       " '디자인': 919,\n",
       " '신종은': 920,\n",
       " '않느냐': 921,\n",
       " '오는': 922,\n",
       " '체중': 923,\n",
       " '찾아가': 924,\n",
       " '반품': 925,\n",
       " 'PK': 926,\n",
       " '이수': 927,\n",
       " '라운드': 928,\n",
       " '동춘당': 929,\n",
       " '40%': 930,\n",
       " '엄격해지자': 931,\n",
       " '의학': 932,\n",
       " '거들': 933,\n",
       " '부시장': 934,\n",
       " '해달라': 935,\n",
       " '쇼케이스': 936,\n",
       " '오피스': 937,\n",
       " '형편': 938,\n",
       " '보통': 939,\n",
       " '패러다임': 940,\n",
       " '전달': 941,\n",
       " '적절': 942,\n",
       " '인류': 943,\n",
       " 'CJ': 944,\n",
       " '서울대': 945,\n",
       " '맘스캠프': 946,\n",
       " '수강': 947,\n",
       " '900만': 948,\n",
       " '도시': 949,\n",
       " '상품': 950,\n",
       " '급부': 951,\n",
       " 'hofer': 952,\n",
       " '번째': 953,\n",
       " '홈런': 954,\n",
       " '붙어': 955,\n",
       " '180': 956,\n",
       " '사파리': 957,\n",
       " '한도': 958,\n",
       " '페투치니': 959,\n",
       " '40': 960,\n",
       " 'WalkON': 961,\n",
       " '언론보도': 962,\n",
       " '즐기는': 963,\n",
       " 'VC': 964,\n",
       " '리틀': 965,\n",
       " '걸린': 966,\n",
       " '불거졌다': 967,\n",
       " '면담': 968,\n",
       " '켜면': 969,\n",
       " '치료': 970,\n",
       " '비롯': 971,\n",
       " '14': 972,\n",
       " '명실': 973,\n",
       " 'AKMU': 974,\n",
       " '이원재': 975,\n",
       " '허영지': 976,\n",
       " '효능': 977,\n",
       " '않게': 978,\n",
       " '선물': 979,\n",
       " '혜원': 980,\n",
       " '구체': 981,\n",
       " '항목': 982,\n",
       " '혈세': 983,\n",
       " '5.3': 984,\n",
       " '매점': 985,\n",
       " '연구': 986,\n",
       " '제로': 987,\n",
       " '비해': 988,\n",
       " '구성': 989,\n",
       " '옮겨가고': 990,\n",
       " '노후': 991,\n",
       " '대상': 992,\n",
       " '항의': 993,\n",
       " '소견': 994,\n",
       " '묘역': 995,\n",
       " '전이': 996,\n",
       " '완화': 997,\n",
       " '막는': 998,\n",
       " '대의': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(docs)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284fba1",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d293b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c02f2",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # 쿼리 은닉 상태(query hidden state)는 (batch_size, hidden size)쌍으로 이루어져 있습니다.\n",
    "        # query_with_time_axis은 (batch_size, 1, hidden size)쌍으로 이루어져 있습니다.\n",
    "        # values는 (batch_size, max_len, hidden size)쌍으로 이루어져 있습니다.\n",
    "        # 스코어(score)계산을 위해 덧셈을 수행하고자 시간 축을 확장하여 아래의 과정을 수행합니다.\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score는 (batch_size, max_length, 1)쌍으로 이루어져 있습니다.\n",
    "        # score를 self.V에 적용하기 때문에 마지막 축에 1을 얻습니다.\n",
    "        # self.V에 적용하기 전에 텐서는 (batch_size, max_length, units)쌍으로 이루어져 있습니다.\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights는 (batch_size, max_length, 1)쌍으로 이루어져 있습니다. \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # 덧셈이후 컨텍스트 벡터(context_vector)는 (batch_size, hidden_size)쌍으로 이루어져 있습니다.\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim, decode_unit, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decode_unit = decode_unit\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec.units,\n",
    "                                      return_sequence=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        slef.attention = BahdanauAttention(self.decode_unit)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "    # enc_output는 (batch_size, max_length, hidden_size)쌍으로 이루어져 있습니다.\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # 임베딩층을 통과한 후 x는 (batch_size, 1, embedding_dim)쌍으로 이루어져 있습니다.\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # 컨텍스트 벡터와 임베딩 결과를 결합한 이후 x의 형태는 (batch_size, 1, embedding_dim + hidden_size)쌍으로 이루어져 있습니다.\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # 위에서 결합된 벡터를 GRU에 전달합니다.\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output은 (batch_size * 1, hidden_size)쌍으로 이루어져 있습니다.\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output은 (batch_size, vocab)쌍으로 이루어져 있습니다.\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762159f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # 교사 강요(teacher forcing) - 다음 입력으로 타겟을 피딩(feeding)합니다.\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # enc_output를 디코더에 전달합니다.\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "        loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # 교사 강요(teacher forcing)를 사용합니다.\n",
    "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee60ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # 에포크가 2번 실행될때마다 모델 저장 (체크포인트)\n",
    "  \n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587a325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e6fc15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.encode_cell = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf34fa2",
   "metadata": {},
   "source": [
    "## Generate Batch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(encoder_inputs, decoder_inputs, targets, target_weights):\n",
    "    encoder_size = len(encoder_inputs[0])\n",
    "    decoder_size = len(decoder_inputs[0])\n",
    "    encoder_inputs, decoder_inputs, targets, target_weights = \\\n",
    "        np.array(encoder_inputs), np.array(decoder_inputs), np.array(targets), np.array(target_weights)\n",
    "    result_encoder_inputs = []\n",
    "    result_decoder_inputs = []\n",
    "    result_targets = []\n",
    "    result_target_weights = []\n",
    "    for i in range(encoder_size):\n",
    "        result_encoder_inputs.append(encoder_inputs[:, i])\n",
    "    for j in range(decoder_size):\n",
    "        result_decoder_inputs.append(decoder_inputs[:, j])\n",
    "        result_targets.append(targets[:, j])\n",
    "        result_target_weights.append(target_weights[:, j])\n",
    "    return result_encoder_inputs, result_decoder_inputs, result_targets, result_target_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b92ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bbc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(object):\n",
    "\n",
    "    def __init__(self, multi, hidden_size, num_layers, forward_only,\n",
    "                 learning_rate, batch_size,\n",
    "                 vocab_size, encoder_size, decoder_size):\n",
    "\n",
    "        # variables\n",
    "        self.source_vocab_size = vocab_size\n",
    "        self.target_vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # networks\n",
    "        W = tf.Variable(tf.random_normal([hidden_size, vocab_size]))\n",
    "        b = tf.Variable(tf.random_normal([vocab_size]))\n",
    "        output_projection = (W, b)\n",
    "        self.encoder_inputs = [tf.placeholder(tf.int32, [batch_size]) for _ in range(encoder_size)]  # 인덱스만 있는 데이터 (원핫 인코딩 미시행)\n",
    "        self.decoder_inputs = [tf.placeholder(tf.int32, [batch_size]) for _ in range(decoder_size)]\n",
    "        self.targets = [tf.placeholder(tf.int32, [batch_size]) for _ in range(decoder_size)]\n",
    "        self.target_weights = [tf.placeholder(tf.float32, [batch_size]) for _ in range(decoder_size)]\n",
    "\n",
    "        # models\n",
    "        if multi:\n",
    "            single_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_size)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "        else:\n",
    "            cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_size)\n",
    "            #cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size)\n",
    "\n",
    "        if not forward_only:\n",
    "            self.outputs, self.states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                self.encoder_inputs, self.decoder_inputs, cell,\n",
    "                num_encoder_symbols=vocab_size,\n",
    "                num_decoder_symbols=vocab_size,\n",
    "                embedding_size=hidden_size,\n",
    "                output_projection=output_projection,\n",
    "                feed_previous=False)\n",
    "\n",
    "            self.logits = [tf.matmul(output, output_projection[0]) + output_projection[1] for output in self.outputs]\n",
    "            self.loss = []\n",
    "            for logit, target, target_weight in zip(self.logits, self.targets, self.target_weights):\n",
    "                crossentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logit, target)\n",
    "                self.loss.append(crossentropy * target_weight)\n",
    "            self.cost = tf.nn.math_ops.add_n(self.loss)\n",
    "            self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "\n",
    "\n",
    "        else:\n",
    "            self.outputs, self.states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                self.encoder_inputs, self.decoder_inputs, cell,\n",
    "                num_encoder_symbols=vocab_size,\n",
    "                num_decoder_symbols=vocab_size,\n",
    "                embedding_size=hidden_size,\n",
    "                output_projection=output_projection,\n",
    "                feed_previous=True)\n",
    "            self.logits = [tf.matmul(output, output_projection[0]) + output_projection[1] for output in self.outputs]\n",
    "\n",
    "    def step(self, session, encoderinputs, decoderinputs, targets, targetweights, forward_only):\n",
    "        input_feed = {}\n",
    "        for l in range(len(encoder_inputs)):\n",
    "            input_feed[self.encoder_inputs[l].name] = encoderinputs[l]\n",
    "        for l in range(len(decoder_inputs)):\n",
    "            input_feed[self.decoder_inputs[l].name] = decoderinputs[l]\n",
    "            input_feed[self.targets[l].name] = targets[l]\n",
    "            input_feed[self.target_weights[l].name] = targetweights[l]\n",
    "        if not forward_only:\n",
    "            output_feed = [self.train_op, self.cost]\n",
    "        else:\n",
    "            output_feed = []\n",
    "            for l in range(len(decoder_inputs)):\n",
    "                output_feed.append(self.logits[l])\n",
    "        output = session.run(output_feed, input_feed)\n",
    "        if not forward_only:\n",
    "            return output[1] # loss\n",
    "        else:\n",
    "            return output[0:] # outputs\n",
    "\n",
    "sess = tf.Session()\n",
    "model = seq2seq(multi=multi, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                    learning_rate=learning_rate, batch_size=batch_size,\n",
    "                    vocab_size=vocab_size,\n",
    "                    encoder_size=encoder_size, decoder_size=decoder_size,\n",
    "                    forward_only=forward_only)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "step_time, loss = 0.0, 0.0\n",
    "current_step = 0\n",
    "start = 0\n",
    "end = batch_size\n",
    "while current_step < 10000001:\n",
    "\n",
    "    if end > len(title):\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "\n",
    "    # Get a batch and make a step\n",
    "    start_time = time.time()\n",
    "    encoder_inputs, decoder_inputs, targets, target_weights = tool.make_batch(encoderinputs[start:end],\n",
    "                                                                              decoderinputs[start:end],\n",
    "                                                                              targets_[start:end],\n",
    "                                                                              targetweights[start:end])\n",
    "\n",
    "    if current_step % steps_per_checkpoint == 0:\n",
    "        for i in range(decoder_size - 2):\n",
    "            decoder_inputs[i + 1] = np.array([word_to_ix['<PAD>']] * batch_size)\n",
    "        output_logits = model.step(sess, encoder_inputs, decoder_inputs, targets, target_weights, True)\n",
    "        predict = [np.argmax(logit, axis=1)[0] for logit in output_logits]\n",
    "        predict = ' '.join(ix_to_word[ix][0] for ix in predict)\n",
    "        real = [word[0] for word in targets]\n",
    "        real = ' '.join(ix_to_word[ix][0] for ix in real)\n",
    "        print('\\n----\\n step : %s \\n time : %s \\n LOSS : %s \\n 예측 : %s \\n 손질한 정답 : %s \\n 정답 : %s \\n----' %\n",
    "              (current_step, step_time, loss, predict, real, title[start]))\n",
    "        loss, step_time = 0.0, 0.0\n",
    "\n",
    "    step_loss = model.step(sess, encoder_inputs, decoder_inputs, targets, target_weights, False)\n",
    "    step_time += time.time() - start_time / steps_per_checkpoint\n",
    "    loss += np.mean(step_loss) / steps_per_checkpoint\n",
    "    current_step += 1\n",
    "    start += batch_size\n",
    "    end += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1994c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#param\n",
    "multi = True\n",
    "forward_only = False\n",
    "hidden_size = 300\n",
    "vocab_size = len(ix_to_word)\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "encoder_size = 100\n",
    "decoder_size = tool.check_doclength(title,sep=True) # (Maximum) number of time steps in this batch\n",
    "steps_per_checkpoint = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \n",
    "title, contents = loading_data(data_path, eng=False, num=False, punc=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
